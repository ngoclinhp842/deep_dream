{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR NAMES HERE**\n",
    "\n",
    "Fall 2024\n",
    "\n",
    "CS 343: Neural Networks\n",
    "\n",
    "Project 4: Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "import tf_util\n",
    "from deep_dream import DeepDream\n",
    "\n",
    "plt.show()\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: DeepDream\n",
    "\n",
    "You will make use of a pre-trained neural network (`VGG19`) to implement the **DeepDream (gradient ascent)** algorithm using TensorFlow to generate art with a neural network! The algorithm projects the receptive fields of specific filters and/or layers onto the input image to create some trippy effects! A neat side effect of this process is that you can visualize how learned weights at different levels of the network interact with parts of the input image.\n",
    "\n",
    "This task will expose you to TensorFlow's low level API that operates on image data in the Tensor data structure.\n",
    "\n",
    "### Overview\n",
    "\n",
    "1. Load in the VGG19 pre-trained network without the output layer.\n",
    "2. Make a readout model â€” a `Keras Model` object that allows us extract activations (`netActs`) within the subset of VGG19 network layers of our choice. Neurons in these layers of the VGG19 network will \"dream\": modify the input image to amplify patterns that the cells \"want to see\" based on their learned weight patterns. \n",
    "3. Select the types of layers (e.g. conv, pooling, etc) that we want to use to run Deep Dream on to influence the input (generated) image.\n",
    "4. Pass image through the readout network model, compute the netAct values and gradient at the selected layers.\n",
    "5. Do gradient ascent where we add a proportion of the gradient from a network layer back into the generated image for some number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Import and plot the image of Miller\n",
    "\n",
    "`miller3_224x224.jpg` is the initial image that will serve as your **generated image**.\n",
    "\n",
    "In the cell below:\n",
    "- Load in the Miller test image `miller3_224x224.jpg` at 224 x 224 resolution. This matches the resolution of images on which VGG19 was trained.\n",
    "- Normalize the image so that pixel values may span (0, 1) rather than (0, 255). Do this by dividing by 255. You should **not** use min-max normalization here (see note below)!\n",
    "- Make a plot of the image.\n",
    "\n",
    "*Why you do not want to use min-max normalization: If you have a dark image that does not have white, min-max normalization will introduce a white value when it was not there originally. This will distort the colors in your image. Dividing by the max possible pixel value (255) works around this issue.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7a44ed12da534c85287ce83d8c17edb",
     "grade": false,
     "grade_id": "cell-990892b6c94a74cc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_img(img, title=''):\n",
    "    plt.imshow(img)\n",
    "    plt.title(title)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep this test code\n",
    "print(f'Image shape is {generated_img.shape} and should be (224, 224, 3)')\n",
    "print(f'Image min/max is: {generated_img.min()}/{generated_img.max()}. It should be 0.0/1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Load in pre-trained VGG19 network.\n",
    "\n",
    "Load in the [pre-trained VGG19 network](https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG19) and set it to a variable called `pretrained_net`. Make sure the existing weights are not trainable and don't load/include the trained output layer.\n",
    "\n",
    "If you call the `summary()` method on the network object, you should see the following at the bottom:\n",
    "\n",
    "    Total params: 20,024,384 (76.39 MB)\n",
    "    Trainable params: 20,024,384 (76.39 MB)\n",
    "    Non-trainable params: 0 (0.00 B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aca4fc311a9c8734e2e0f0081794aa60",
     "grade": false,
     "grade_id": "cell-eb81edb90123ef8d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Write `load_pretrained_net` to automate loading of pretrained network\n",
    "\n",
    "In `tf_util.py`, adapt your code from the previous subtask to write the `load_pretrained_net` function. This function automates the process of loading the pretrained network and paves the way for possibly loading in pre-trained networks other than VGG19.\n",
    "\n",
    "Call your function in the cell below. Calling the `summary` method on the returned pretrained network object should produce the same output as in Task 3b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20cdcb3c10fd2277c2ffd1001721f2ba",
     "grade": false,
     "grade_id": "cell-6adc619285b41cf6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. Create a list of `VGG19` network layer names and those whose activation we want to sample\n",
    "\n",
    "In `tf_util.py`, implement the following functions:\n",
    "- `get_all_layer_strs(pretrained_net)`: Extract a list of strings from the pretrained network that represents all the layer names.\n",
    "- `filter_layer_strs(layer_names, match_str='block5')`: Filter out layer names from the complete list that have a certain substring. For example, return the subset of layers that have `'block5'` in their name. This is the list of **selected layers** whose activity we will amplify by adding their gradient back into the input image (i.e. these layers will \"do the dreaming\").\n",
    "\n",
    "Run the following test code below to check your implementations. Each function should be short (~1-5 lines of code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all layer names\n",
    "layer_names = tf_util.get_all_layer_strs(pretrained_net)\n",
    "print(f'Your full list of layers contains {len(layer_names)} names and it should contain 22.')\n",
    "print(f'Your first layer name is {layer_names[0]} and it should be input_1.')\n",
    "print(' It is not a problem if you have a different int after the underscore (e.g. input_3)')\n",
    "print(f'Your last layer name is {layer_names[-1]} and it should be block5_pool.')\n",
    "\n",
    "print()\n",
    "selected_layer_names = tf_util.filter_layer_strs(layer_names, match_str='block5')\n",
    "print(f'Your selected layer names are:\\n{selected_layer_names}\\nand they should be')\n",
    "print(\"['block5_conv1', 'block5_conv2', 'block5_conv3', 'block5_conv4', 'block5_pool']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3e. Preprocess initial generated image\n",
    "\n",
    "In `tf_util.py`, implement the function `preprocess_image2tf(img, as_var)`. This function takes the initial generated image (e.g. `miller3.jpg`) in Numpy ndarray format and converts it to a TensorFlow Variable tensor.\n",
    "\n",
    "In the cell below, call your function to preprocess the test image of Miller as a TensorFlow Variable. Name the preprocessed image `generated_img_tf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aaa0d1ab339473cec7e8374cbd3e28cd",
     "grade": false,
     "grade_id": "cell-5779af9ab0e21311",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep below test code\n",
    "print(f'The shape of the initial generated image is {generated_img_tf.shape} and should be (1, 224, 224, 3)')\n",
    "print(f'The min/max of the initial generated image is {tf.reduce_min(generated_img_tf)}/{tf.reduce_max(generated_img_tf)} and should be 0.0/1.0')\n",
    "print(f\"The datatype of the initial generated image is {generated_img_tf.dtype} and should be <dtype: 'float32'>\")\n",
    "print(f'The initial generated image is trainable? {hasattr(generated_img_tf, \"trainable\")}. It should be!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3f. Build VGG19 selected layer readout model\n",
    "\n",
    "When we do the forward pass with the generated image (e.g. `miller3`), we get activations in every network layer. However, we only want a select group of layers to influence the input image (e.g. only those with `block5` in the name). We could in principle take the list of ALL the netActs and extract the netActs we want according to indices of the desired layers, but there is more overhead with this approach since there are a lot of netAct values we won't use! It is more convenient to build a `tf.keras.Model` object based on the pretrained network that returns a list of netActs *in only the layers we specify*. We will call this model that returns the subset of layer netActs that we care about **the readout model**.\n",
    "\n",
    "In `tf_util.py`, implement the function `make_readout_model(pretrained_net, layer_names)` that builds the readout model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_readout_model = tf_util.make_readout_model(pretrained_net, selected_layer_names)\n",
    "\n",
    "rng = tf.random.Generator.from_seed(0)\n",
    "test_input = rng.uniform(shape=(1, 224, 224, 3))\n",
    "test_net_acts = test_readout_model(test_input)\n",
    "print(f'Number of layers of netAct returned is {len(test_net_acts)} and should be 5')\n",
    "print(f'Shape of first layer of netActs is {test_net_acts[0].shape} and should be (1, 14, 14, 512)')\n",
    "print(f'First few netAct values of first layer filters in top-left corner:\\n{test_net_acts[0][0,0,0,:5]}')\n",
    "print('and they should be:')\n",
    "print('[0.179 0.    2.947 0.    0.   ]')\n",
    "print(f'Shape of last layer of netActs is {test_net_acts[-1].shape} and should be (1, 7, 7, 512)')\n",
    "print(f'First few netAct values of last layer filters in top-left corner:\\n{test_net_acts[-1][0,0,0,:5]}')\n",
    "print('and they should be:')\n",
    "print('[0.399 0.    0.    0.    0.699]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3g. Start implementing Deep Dream network\n",
    "\n",
    "Implement the following `DeepDream` class methods in `deep_dream.py`:\n",
    "- Constructor\n",
    "- `loss_layer(self, layer_net_acts)`: Computes one selected layer's contribution to the total loss. This is the mean activation (`netAct`) across the layer.\n",
    "- `forward(self, gen_img, eps=1e-8, standardize_grads=True)`: Does the forward pass with the generated image, computes the total loss, and computes the gradient of the loss with respect to the input generated image. If `standardize_grads` is set to `True`, the image gradients should be standardized according to the usual formula (see below).\n",
    "\n",
    "#### Image gradient standardization\n",
    "\n",
    "The following equation computes the standardized image gradient $\\hat{g}_{i,j}$ at pixel $(i,j)$ in the image gradient.\n",
    "\n",
    "$$\n",
    "\\hat{g}_{i,j} = \\frac{g_{i,j} - \\mu_g}{\\sigma_g + \\epsilon}\n",
    "$$\n",
    "\n",
    "where $g_{i,j}$ is the original/raw gradient of the image $(i,j)$ with respect to the total loss (i.e. `d_image`), $\\mu_g$ is the mean of the image gradients, $\\sigma_g$ is the standard deviation of the image gradients, and $\\epsilon$ a small numer (e.g. `1e-8`) to prevent possible division by 0.\n",
    "\n",
    "Notes:\n",
    "- Remember that the shape of `d_thing` has the same shape as `thing`. So `d_image` has shape `(224, 224, 3)` â€” every pixel in the original image gets a gradient update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Test `loss_layer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dd = DeepDream(pretrained_net, selected_layer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = tf.random.Generator.from_seed(0)\n",
    "test_layer_net_acts = rng.uniform(shape=(1, 14, 14, 512))\n",
    "test_layer_loss = test_dd.loss_layer(test_layer_net_acts)\n",
    "print(f'Your test layer loss is {test_layer_loss:.5f} and should be 0.49952')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Test `forward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = tf.random.Generator.from_seed(0)\n",
    "test_img_tf = tf.Variable(rng.uniform(shape=(1, 224, 224, 3), minval=0, maxval=1))\n",
    "test_loss, test_grads = test_dd.forward(test_img_tf)\n",
    "print(f'Your total loss is {test_loss:.5f} and should be 0.26621')\n",
    "print(f'The shape of your image gradients are {test_grads.shape} and it should be (1, 224, 224, 3)')\n",
    "print(f'The first few gradients are\\n{test_grads[0,0,:5,0]}\\nand they should be\\n[-0.102  0.398  0.599  0.388  0.71 ]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3h. Implement `fit`: use gradient ascent algorithm to iteratively modify generated image\n",
    "\n",
    "The `fit` method uses **gradient ascent** to update the generated image on every epoch â€” i.e. the network modifies the input image across epochs based on the image gradients. The result is a surreal, trippy, dreamy effect that merges the neural and image representations.\n",
    "\n",
    "#### Gradient ascent\n",
    "\n",
    "$$\n",
    "I(t+1) = I(t) + \\alpha \\times \\hat{g}\n",
    "$$\n",
    "\n",
    "where $I$ is the generated image on epoch $t$, $\\alpha$ is the learning rate, and $\\hat{g}$ is the image gradient on the current epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: Deep Dream for 1 epoch\n",
    "\n",
    "The code below runs `fit` for one epoch.\n",
    "- You should print out the estimated amount of time in minutes to complete `E` epochs.\n",
    "- You should print out how many epochs have been completed out of `E` total. This should happen on the first epoch (and in general `print_every` epochs).\n",
    "- `fit` should generate a plot showing the generated image after 1 epoch. *You should see cool shapes starting to emerge in the generated image*! To help make this happen, implement `tf2image(tensor)` in `tf_util.py` that converts the TensorFlow tensor into a PIL `Image` object.\n",
    "- You should export a JPG of the generated image in the `deep_dream_output` folder within your project working directory (*create it if it doesn't exist*). \n",
    "\n",
    "Here is an example progress printout with the estimated time:\n",
    "\n",
    "```\n",
    "Epoch 0/0 completed\n",
    "  1 epoch took 0.010 mins. Expected runtime is 0.010 mins.\n",
    "```\n",
    "\n",
    "*Note: the `tf.identity` makes and returns a copy of `generated_img_tf` â€” this way we do not modify the original Tensor of the image during this test.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_img_1epoch = tf.Variable(tf.identity(generated_img_tf))\n",
    "loss_hist = test_dd.fit(gen_img_1epoch, n_epochs=1, lr=0.1)\n",
    "print('Test code:')\n",
    "print(f'Your loss over 1 epoch is {loss_hist[0]:.1f} and should be 0.4')\n",
    "print(f'Generated image min/max pixel values are {tf.reduce_min(gen_img_1epoch)}/{tf.reduce_max(gen_img_1epoch)} and should be 0.0/1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3i. Run Deep Dream on `miller3_224x224.jpg`\n",
    "\n",
    "In the cell below, copy-and-paste code that you've written above to:\n",
    "- Load in and preprocess the initial generated image (`miller3_224x224.jpg` here).\n",
    "- Load in the pretrained VGG19 network.\n",
    "- Creating a `DeepDream` network and fitting it to the image over `26` epochs with the default learning rate of `0.01`.\n",
    "- Only activations in layers with `block5` in the name should contribute to the loss and lead to modifications of the generated image.\n",
    "\n",
    "The goal is to run the complete Deep Dream algorithm on a generated image without having to execute all the cells above.\n",
    "\n",
    "The cell below should show `miller3` after running fit for 26 epochs. *Even if your computer is a few years old, this should take at most 5 minutes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9dbdeb9d8669f04bc11860b04619ed4b",
     "grade": false,
     "grade_id": "cell-835442a505e3b513",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3j. Refine Deep Dream by running it with multiple image scales\n",
    "\n",
    "While the existing Deep Dream algorithm results in some cool effects, a little additional work can make the Deep Dream images look even better.\n",
    "\n",
    "Implement `fit_multiscale` in `deep_dream.py`. In this version you call `fit` to run the gradient ascent algorithm on the generated image over multiple runs. Each time you finish a run, scale (resize) the image so that you run gradient ascent again on the larger version of image.\n",
    "\n",
    "Since this is effectively repeating `gradient_ascent` a set number of times (`n_scales`), it will take longer to finish computing.\n",
    "\n",
    "#### Test `fit_multiscale`\n",
    "\n",
    "Copy-and-paste your code above from Task 3i, but now run `fit_multiscale` instead of `fit`. Run it with default parameters for now. The cell below should:\n",
    "- show the generated image (`miller3`) after gradient ascent is run on the generated image at each scale. \n",
    "- print outs showing the progress (i.e. number of scales currently completed).\n",
    "- Estimated total runtime based on the first scale.\n",
    "\n",
    "*Even if your computer is several years old, this should take at most 5-10 mins to finish. If it is taking too long and you think that your code is working properly, run it in Davis 102 or cut down on the number of epochs and scales.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55ab29244a356edeb1f61d6b0369b683",
     "grade": false,
     "grade_id": "cell-b7a46e0c6fede57e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3k. Run Deep Dream on at least one other image\n",
    "\n",
    "Copy-and-paste your code again and run either the single-scale or multi-scale version of Deep Dream on another image of your choice. It could be one of the other provided images. Adjust hyperparameters as necessary so that the generated image bears some resemblance to the original image (i.e. the generated image is not totally dominated by the network influence).\n",
    "\n",
    "**Note about the size of your image:**\n",
    "- You can resize your image to 224x224, the resolution of images VGG19 was trained on.\n",
    "- If you have a more powerful machine (or are willing to wait longer for prettier pictures), you can use higher resolution images. But you should start with the 224 x 224 version for debugging purposes. You can use the PIL Image resize method. Alternatively, there are numerous external tools (e.g. ImageMagick) and other Python-based ways to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "042f27fc3e4161b8c4815aa624c69d77",
     "grade": false,
     "grade_id": "cell-a90c06ece408cae7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3l. Questions\n",
    "\n",
    "Generate one or more Deep Dream images to accompany each of your written answers to the following questions. You can include them inline with your answers below.\n",
    "\n",
    "**Question 6:** Why does the multi-scale version tends to produce better visualizations? *Hint: think about the filter receptive fields involved in the process.*\n",
    "\n",
    "**Question 7**: What is one key difference in the generated images when only the earlier/later network layers are involved in the Deep Dream process? **Make 1 or more images that illustrate your point.**\n",
    "\n",
    "**Question 8**: What is one key difference in the generated images when you let pooling vs different conv layers contribute to the generated image.  **Make 1 or more images that illustrate your point.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 6:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4546af0a032bbc128503e0708ac2be6c",
     "grade": true,
     "grade_id": "cell-ebbf552262ec76c0",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 7**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eda1912b91c2a2ba250a7ed773ca4909",
     "grade": true,
     "grade_id": "cell-bec4f8298048915f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 8**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4fe75d09187580779481ec59c44d716f",
     "grade": true,
     "grade_id": "cell-f7521dc63ad190e0",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "### General guidelines\n",
    "\n",
    "1. Never integrate extensions into your base project so that they change the expected behavior of core functions. If your extension changes the core design/behavior, no problem, duplicate your working base project and add features from there.\n",
    "2. Check the rubric to keep in mind how extensions on this project will be graded.\n",
    "3. While I may consult your code and \"written log\" of what you did, **I am grading your extensions based on what you present in your 3-5 min video.**\n",
    "3. I suggest documenting your explorations in a \"log\" or \"lab notebook\" style (i.e. documenting your thought/progression/discovery/learning process). I'm not grading your writing, so you can keep it succinct. **Whatever is most useful to you to remember what you did.** \n",
    "4. I suggest taking a hypothesis driven approach. For example \"I was curious about X so I explored Y. I found Z, which was not what I expected because..., so then tried A...\"\n",
    "5. Make plots to help showcase your results.\n",
    "6. **More is not necessarily better.** Generally, a small number of \"in-depth\" extensions count for more than many \"shallow\" extensions.\n",
    "\n",
    "### AI guidelines\n",
    "\n",
    "You may use AI in mostly any capacity for extensions. However, keep in mind:\n",
    "1. There is no need to use AI at all!\n",
    "2. You are welcome to use AI as a tool (e.g. automate something that is tedious, help you get unstuck, etc.). However, you should be coding, you should be thinking, you should be writing, you should be creating. If you are spending most (or even close to most) of your time typing into a chatbot and copy-pasting, you have probably gone too far with AI use.\n",
    "3. I don't find large volumes of AI generated code/text/plots to be particularly impressive and you risk losing my interest while grading. Remember: I'm grading your extensions based on your video presentation. **More is not necessarily better.**\n",
    "\n",
    "### Video guidelines\n",
    "\n",
    "1. Please try to keep your video to 5 minutes (*I have other projects to grade!*). If you turn in a longer video, I make no promise that I will watch more than 5 minutes.\n",
    "2. Your screen should be shared as you show me what you did. A live video of your face should also appear somewhere on the screen (e.g. picture-in-picture overlay / split screen).\n",
    "3. Your partner should join you for the video and take turns talking, but, if necessary, it is fine to have one team member present during the record the video.\n",
    "4. Do not simply read text from your notebook, do not read from a prepared script. I am not grading how polished your video presentation is (see extension grading criteria on rubric). \n",
    "5. I am looking for original and creative explorations sparked by your curiosity/interest/passion in a topic. This should be apparent in your video.\n",
    "6. Be natural,, don't feel the need to impress me with fancy language. If it is helpful, imagine that we are talking one-on-one about your extension. Tell me what you did :)\n",
    "\n",
    "### Extension ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Run DeepDream on your own images. Make some interesting effects.\n",
    "\n",
    "Change hyperparameters as needed to get cool results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Research other pretrained networks in TensorFlow and run DeepDream on their layers/filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualize individual CNN filters\n",
    "\n",
    "- Rather than having the netActs from each entire layer influence the generated image, only have the netAct values of specific filters influence the generated image. This feature is is potentially interesting/useful for analysis because you can visualize what shape/object/animal/texture a particular filter learns (e.g. filter at index 0 learns seals, filter at index 1 learns birds, filter at index 2 learns fish, etc). This would involve defining a list of indices to sample within all or specific layers.\n",
    "- You can use this approach to visualize the features that a CNN filter learns at any depth of the network. Instead of presenting the network with an actual image, apply Deep Dream to an image of RGB noise. The selected filter will amplify patterns/textures that are consistent with those to which it is tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Transfer learning: What's the best test accuracy that you can achieve on the hot dog or not dataset? How did you do it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Keras ConvNet4: Explore and analyze how different network architectures affect the STL-10 results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Transfer learning: Can you apply Transfer learning to a binary or multi-class classification task with another dataset of your choice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
